{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "f30eb3b7-dbf6-4c48-adee-9610f3cd58d0"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "74cdc08c-592b-4737-bd37-cd75401707b6"
   },
   "outputs": [],
   "source": [
    "# models\n",
    "discrete_encoder_path = 'discrete_encoder.pth'\n",
    "discrete_decoder_path = 'discrete_decoder.pth'\n",
    "mapping_path = 'mapping.pth'\n",
    "unet_path = 'con_unet_full.pth'\n",
    "continuous_model_path = 'continuous_model.pth'\n",
    "\n",
    "hidden_dim_discrete = 128\n",
    "\n",
    "# data preparation\n",
    "data_file = '/home/ankbzpx/datasets/ShapeNet/ShapeNetRenderingh5_v1/03001627/sdf_train_core.h5'\n",
    "sample_size = 2048\n",
    "batch_size = 32\n",
    "split_ratio = 0.9\n",
    "depth_size = 256\n",
    "num_of_workers = 12\n",
    "# training\n",
    "num_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "c10b2893-a503-482e-95bb-2554e96864ba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "6b4c7bac-058e-4eb6-b0cd-a9f21f24ad9e"
   },
   "outputs": [],
   "source": [
    "# reproducible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "b8cefb09-7a78-4ae7-b9ea-001858c59003"
   },
   "outputs": [],
   "source": [
    "class ChairSDFDataset(Dataset):\n",
    "     \n",
    "    def __init__(self, h5_file):\n",
    "        \n",
    "        self.file_path = h5_file\n",
    "        self.dataset = None\n",
    "        \n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.dataset_len = len(file)\n",
    "            self.keys = list(file.keys())\n",
    "            \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "     \n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            self.dataset = h5py.File(self.file_path, 'r')\n",
    "         \n",
    "        group = self.dataset[self.keys[idx]]\n",
    "        \n",
    "        depth_img = self.to_tensor(Image.fromarray(np.array(group['depth_img'])))\n",
    "        \n",
    "        #print(\"--- depth preprocessing %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        sample_pt_np = np.array(group['sample_pt']).reshape(-1, 3)\n",
    "        sample_sdf_np = np.array(group['sample_sdf']).reshape(-1, 1)\n",
    "        \n",
    "        # check size correctness and fix incorrect data\n",
    "        if sample_pt_np.shape[0] != 2048:\n",
    "            sample_pt_np = np.pad(sample_pt_np, ((0, 2048 - sample_pt_np.shape[0]), (0, 0)), 'reflect')\n",
    "        if sample_sdf_np.shape[0] != 2048:\n",
    "            sample_sdf_np = np.pad(sample_sdf_np, ((0, 2048 - sample_sdf_np.shape[0]), (0, 0)), 'reflect')\n",
    "            \n",
    "        \n",
    "        sample_pt = torch.from_numpy(sample_pt_np).float()\n",
    "        sample_sdf = torch.from_numpy(sample_sdf_np).float()\n",
    "        # scale sdf\n",
    "        sample_sdf = torch.sign(sample_sdf)*torch.pow(torch.abs(sample_sdf), 0.25)\n",
    "        \n",
    "        #print(\"--- subsampling %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        target_vox = torch.from_numpy(np.array(group['target_vox'])).float()\n",
    "        \n",
    "        sample = { 'depth_img': depth_img,\n",
    "                   'sample_pt':sample_pt,\n",
    "                   'sample_sdf':sample_sdf,\n",
    "                   'target_vox':target_vox,\n",
    "                  }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "2f44af65-5139-4d46-ac93-2f620eaceb79"
   },
   "outputs": [],
   "source": [
    "train_sdf_dataset = ChairSDFDataset(data_file)\n",
    "\n",
    "train_sdf_dataloader = DataLoader(train_sdf_dataset, batch_size=batch_size, shuffle=True, num_workers=num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2634"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sdf_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "latent_dim = 256\n",
    "\n",
    "from models import Discrete_encoder, Mapping, Discrete_decoder, Conditional_UNET\n",
    "\n",
    "####################\n",
    "# Discrete Encoder #\n",
    "####################\n",
    "\n",
    "discrete_encoder = Discrete_encoder(256).to(device)\n",
    "discrete_encoder.load_state_dict(torch.load(discrete_encoder_path))\n",
    "discrete_encoder.eval()\n",
    "\n",
    "for child in discrete_encoder.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "####################\n",
    "# Mapping #\n",
    "####################\n",
    "\n",
    "mapping = Mapping().to(device)\n",
    "mapping.load_state_dict(torch.load(mapping_path))\n",
    "mapping.eval()\n",
    "\n",
    "for child in mapping.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# ####################\n",
    "# # Discrete Decoder #\n",
    "# ####################\n",
    "        \n",
    "discrete_decoder = Discrete_decoder(256).to(device)\n",
    "discrete_decoder.load_state_dict(torch.load(discrete_decoder_path))\n",
    "discrete_decoder.eval()\n",
    "\n",
    "for child in discrete_decoder.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "########\n",
    "# UNET #\n",
    "########\n",
    "\n",
    "# pre-trained model is loaded within the model\n",
    "unet = Conditional_UNET(unet_path).to(device)\n",
    "unet.eval()\n",
    "\n",
    "for child in unet.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNect1D(nn.Module):\n",
    "    def __init__(self, input_dim, expand = 5):\n",
    "        super(BottleNect1D, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, expand*input_dim),\n",
    "            nn.BatchNorm1d(expand*input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expand*input_dim, input_dim),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Continuous(nn.Module):\n",
    "    def __init__(self, pt_dim = 3, con_dim = 32, latent_dim = 256):\n",
    "        super(Continuous, self).__init__()\n",
    "        \n",
    "        self.de_pt =  nn.Sequential(\n",
    "            nn.Linear(pt_dim + con_dim + latent_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            BottleNect1D(latent_dim),\n",
    "        )\n",
    "        \n",
    "        self.de_1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            BottleNect1D(latent_dim),\n",
    "        )\n",
    "        \n",
    "        self.de_2 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            BottleNect1D(latent_dim),\n",
    "            nn.Linear(latent_dim, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pt, con, z):\n",
    "        \n",
    "        fea = self.de_pt(torch.cat((torch.cat((pt, con), 1), z), 1))\n",
    "        out = self.de_1(fea) + fea\n",
    "        out = self.de_2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous = Continuous().to(device)\n",
    "continuous.load_state_dict(torch.load(continuous_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]          74,752\n",
      "       BatchNorm1d-2                  [-1, 256]             512\n",
      "            Linear-3                 [-1, 1280]         328,960\n",
      "       BatchNorm1d-4                 [-1, 1280]           2,560\n",
      "              ReLU-5                 [-1, 1280]               0\n",
      "            Linear-6                  [-1, 256]         327,936\n",
      "       BatchNorm1d-7                  [-1, 256]             512\n",
      "      BottleNect1D-8                  [-1, 256]               0\n",
      "            Linear-9                  [-1, 256]          65,792\n",
      "      BatchNorm1d-10                  [-1, 256]             512\n",
      "           Linear-11                 [-1, 1280]         328,960\n",
      "      BatchNorm1d-12                 [-1, 1280]           2,560\n",
      "             ReLU-13                 [-1, 1280]               0\n",
      "           Linear-14                  [-1, 256]         327,936\n",
      "      BatchNorm1d-15                  [-1, 256]             512\n",
      "     BottleNect1D-16                  [-1, 256]               0\n",
      "           Linear-17                  [-1, 256]          65,792\n",
      "      BatchNorm1d-18                  [-1, 256]             512\n",
      "           Linear-19                 [-1, 1280]         328,960\n",
      "      BatchNorm1d-20                 [-1, 1280]           2,560\n",
      "             ReLU-21                 [-1, 1280]               0\n",
      "           Linear-22                  [-1, 256]         327,936\n",
      "      BatchNorm1d-23                  [-1, 256]             512\n",
      "     BottleNect1D-24                  [-1, 256]               0\n",
      "           Linear-25                    [-1, 1]             257\n",
      "             Tanh-26                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,188,033\n",
      "Trainable params: 2,188,033\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 8.35\n",
      "Estimated Total Size (MB): 8.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(continuous, [(3, ), (32, ), (256, )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "66f2e463-b1d4-4d52-bd69-503dee235960"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "model = continuous\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state):\n",
    "    torch.save(state, 'continuous_model_checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('continuous_model_checkpoint.pth.tar')\n",
    "start_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['continuous_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced indexing 2x2x2 context from voxel\n",
    "def getContext(sample_pt_query, vox):\n",
    "    \n",
    "    # sample_pt bxmx3\n",
    "    # vox bxcxdimxdimxdim\n",
    "    \n",
    "    channel_size = vox.shape[1]\n",
    "    batch_size, sample_size, _ = sample_pt_query.shape\n",
    "    meshgrid_base = torch.Tensor(np.meshgrid(np.arange(0, batch_size), np.arange(0, channel_size), np.arange(0, 2), np.arange(0, 2), np.arange(0, 2))).int()\n",
    "    context = torch.empty((batch_size, sample_size, channel_size, 2, 2, 2))\n",
    "    \n",
    "    for j in range(context.shape[1]):\n",
    "        context[:, j, :, :, :, :] = vox[\n",
    "                    meshgrid_base[0].long(),\n",
    "                    meshgrid_base[1].long(),\n",
    "                    (meshgrid_base[2] + sample_pt_query[:, j, 0].reshape(1, -1, 1, 1, 1)).long(), \n",
    "                    (meshgrid_base[3] + sample_pt_query[:, j, 1].reshape(1, -1, 1, 1, 1)).long(), \n",
    "                    (meshgrid_base[4] + sample_pt_query[:, j, 2].reshape(1, -1, 1, 1, 1)).long()\n",
    "                ].transpose(0, 1)\n",
    "    \n",
    "    # b x c x m x 2 x 2 x 2\n",
    "    return context.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trilinearInterpolation(context, dx, dy, dz):\n",
    "    \n",
    "    v0 = context[:, :, :, 0, 0, 0]*(1-dx)*(1-dy)*(1-dz)\n",
    "    v1 = context[:, :, :, 1, 0, 0]*dx*(1-dy)*(1-dz)\n",
    "    v2 = context[:, :, :, 0, 1, 0]*(1-dx)*dy*(1-dz)\n",
    "    v3 = context[:, :, :, 1, 1, 0]*dx*dy*(1-dz)\n",
    "    v4 = context[:, :, :, 0, 0, 1]*(1-dx)*(1-dy)*dz\n",
    "    v5 = context[:, :, :, 1, 0, 1]*dx*(1-dy)*dz\n",
    "    v6 = context[:, :, :, 0, 1, 1]*(1-dx)*dy*dz\n",
    "    v7 = context[:, :, :, 1, 1, 1]*dx*dy*dz\n",
    "    \n",
    "    # b x c x m 1\n",
    "    return v0 + v1 + v2 + v3 + v4 + v5 + v6 + v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Epoch:  25\n",
      "Batch:  10 , l1 Loss:  0.09254280274564569 , Time: 17.20931100845337 s\n",
      "Batch:  20 , l1 Loss:  0.09253654256463051 , Time: 30.998764753341675 s\n",
      "Batch:  30 , l1 Loss:  0.09230871349573136 , Time: 45.400079011917114 s\n",
      "Batch:  40 , l1 Loss:  0.09249885976314545 , Time: 58.660292625427246 s\n",
      "Batch:  50 , l1 Loss:  0.09258602485060692 , Time: 73.28973507881165 s\n",
      "Batch:  60 , l1 Loss:  0.09254356026649475 , Time: 87.38306188583374 s\n",
      "Batch:  70 , l1 Loss:  0.09247088059782982 , Time: 100.73967361450195 s\n",
      "Batch:  80 , l1 Loss:  0.09226948395371437 , Time: 116.43030500411987 s\n",
      "Batch:  90 , l1 Loss:  0.09188556298613548 , Time: 130.42106103897095 s\n",
      "Batch:  100 , l1 Loss:  0.0919592835009098 , Time: 144.8918879032135 s\n",
      "Batch:  110 , l1 Loss:  0.09223691299557686 , Time: 158.51891708374023 s\n",
      "Batch:  120 , l1 Loss:  0.09238492324948311 , Time: 172.48464608192444 s\n",
      "Batch:  130 , l1 Loss:  0.09232599958777428 , Time: 187.02431297302246 s\n",
      "Batch:  140 , l1 Loss:  0.09274891838431358 , Time: 200.68533062934875 s\n",
      "Batch:  150 , l1 Loss:  0.09263800829648972 , Time: 215.335999250412 s\n",
      "Batch:  160 , l1 Loss:  0.09254695922136306 , Time: 230.27925968170166 s\n",
      "Batch:  170 , l1 Loss:  0.09248112812638283 , Time: 244.38828134536743 s\n",
      "Batch:  180 , l1 Loss:  0.09244253262877464 , Time: 258.9079473018646 s\n",
      "Batch:  190 , l1 Loss:  0.09260716810822486 , Time: 272.7869699001312 s\n",
      "Batch:  200 , l1 Loss:  0.0917725183069706 , Time: 287.50545740127563 s\n",
      "Batch:  210 , l1 Loss:  0.09269505962729455 , Time: 301.4646055698395 s\n",
      "Batch:  220 , l1 Loss:  0.09208766594529152 , Time: 316.31032943725586 s\n",
      "Batch:  230 , l1 Loss:  0.09250139743089676 , Time: 330.69468927383423 s\n",
      "Batch:  240 , l1 Loss:  0.09274440109729767 , Time: 344.1547620296478 s\n",
      "Batch:  250 , l1 Loss:  0.09187855273485183 , Time: 359.6615126132965 s\n",
      "Batch:  260 , l1 Loss:  0.09215160086750984 , Time: 373.4158205986023 s\n",
      "Batch:  270 , l1 Loss:  0.09246303886175156 , Time: 388.28064465522766 s\n",
      "Batch:  280 , l1 Loss:  0.09257136508822442 , Time: 402.44136214256287 s\n",
      "Batch:  290 , l1 Loss:  0.09259620383381843 , Time: 415.95452213287354 s\n",
      "Batch:  300 , l1 Loss:  0.09215390384197235 , Time: 430.9532344341278 s\n",
      "Batch:  310 , l1 Loss:  0.09237707778811455 , Time: 445.00052762031555 s\n",
      "Batch:  320 , l1 Loss:  0.09241937324404717 , Time: 459.72715067863464 s\n",
      "Batch:  330 , l1 Loss:  0.09249175265431404 , Time: 474.24892568588257 s\n",
      "Batch:  340 , l1 Loss:  0.09204885810613632 , Time: 488.41182827949524 s\n",
      "Batch:  350 , l1 Loss:  0.09245954528450966 , Time: 503.41755747795105 s\n",
      "Batch:  360 , l1 Loss:  0.09242594838142396 , Time: 517.318943977356 s\n",
      "Batch:  370 , l1 Loss:  0.09293714612722397 , Time: 533.0490100383759 s\n",
      "Batch:  380 , l1 Loss:  0.09260012656450271 , Time: 547.9760885238647 s\n",
      "Batch:  390 , l1 Loss:  0.09281933382153511 , Time: 562.9892156124115 s\n",
      "Batch:  400 , l1 Loss:  0.09296218380331993 , Time: 577.7314641475677 s\n",
      "Batch:  410 , l1 Loss:  0.09282280281186103 , Time: 591.5950515270233 s\n",
      "Batch:  420 , l1 Loss:  0.09287756979465485 , Time: 606.5969507694244 s\n",
      "Batch:  430 , l1 Loss:  0.09227516576647758 , Time: 620.9026212692261 s\n",
      "Batch:  440 , l1 Loss:  0.09167045280337334 , Time: 636.2426950931549 s\n",
      "Batch:  450 , l1 Loss:  0.09319335818290711 , Time: 650.9524819850922 s\n",
      "Batch:  460 , l1 Loss:  0.09234903827309608 , Time: 665.3742411136627 s\n",
      "Batch:  470 , l1 Loss:  0.09287185445427895 , Time: 680.8738613128662 s\n",
      "Batch:  480 , l1 Loss:  0.09294322431087494 , Time: 695.0046043395996 s\n",
      "Batch:  490 , l1 Loss:  0.0924154594540596 , Time: 709.8836445808411 s\n",
      "Batch:  500 , l1 Loss:  0.0923007033765316 , Time: 724.6102101802826 s\n",
      "Batch:  510 , l1 Loss:  0.09236300811171531 , Time: 739.4808888435364 s\n",
      "Batch:  520 , l1 Loss:  0.09260562434792519 , Time: 754.938985824585 s\n",
      "Batch:  530 , l1 Loss:  0.09202310368418694 , Time: 769.1356666088104 s\n",
      "Batch:  540 , l1 Loss:  0.0923695519566536 , Time: 784.4942810535431 s\n",
      "Batch:  550 , l1 Loss:  0.09261769130825996 , Time: 799.1207015514374 s\n",
      "Batch:  560 , l1 Loss:  0.0928725428879261 , Time: 813.5003128051758 s\n",
      "Batch:  570 , l1 Loss:  0.09239389523863792 , Time: 829.0984623432159 s\n",
      "Batch:  580 , l1 Loss:  0.09242630079388618 , Time: 843.9175033569336 s\n",
      "Batch:  590 , l1 Loss:  0.09206922352313995 , Time: 859.9795286655426 s\n",
      "Batch:  600 , l1 Loss:  0.09188538268208504 , Time: 874.4875349998474 s\n",
      "Batch:  610 , l1 Loss:  0.091680858284235 , Time: 889.7403390407562 s\n",
      "Batch:  620 , l1 Loss:  0.09309092983603477 , Time: 905.1790716648102 s\n",
      "Batch:  630 , l1 Loss:  0.0923617236316204 , Time: 919.4052836894989 s\n",
      "Batch:  640 , l1 Loss:  0.09216874465346336 , Time: 935.1331324577332 s\n",
      "Batch:  650 , l1 Loss:  0.09211825206875801 , Time: 949.9189412593842 s\n",
      "Batch:  660 , l1 Loss:  0.09209229871630668 , Time: 965.2142868041992 s\n",
      "Batch:  670 , l1 Loss:  0.0927151843905449 , Time: 979.9033846855164 s\n",
      "Batch:  680 , l1 Loss:  0.09272844791412353 , Time: 994.374773979187 s\n",
      "Batch:  690 , l1 Loss:  0.09182720631361008 , Time: 1009.9420580863953 s\n",
      "Batch:  700 , l1 Loss:  0.09203633219003678 , Time: 1024.530324935913 s\n",
      "Batch:  710 , l1 Loss:  0.0922665186226368 , Time: 1040.2790939807892 s\n",
      "Batch:  720 , l1 Loss:  0.09253291264176369 , Time: 1055.591633796692 s\n",
      "Batch:  730 , l1 Loss:  0.09320415034890175 , Time: 1071.9284698963165 s\n",
      "Batch:  740 , l1 Loss:  0.092432901263237 , Time: 1088.1755089759827 s\n",
      "Batch:  750 , l1 Loss:  0.09266232103109359 , Time: 1103.0235795974731 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1bf67f6e20d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msample_pt_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_pt_scale\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msample_pt_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_pt_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvox_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_pt_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4e399b88e063>\u001b[0m in \u001b[0;36mgetContext\u001b[0;34m(sample_pt_query, vox)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         context[:, j, :, :, :, :] = vox[\n\u001b[0;32m---> 14\u001b[0;31m                     \u001b[0mmeshgrid_base\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0mmeshgrid_base\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mmeshgrid_base\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_pt_query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "#test sub module\n",
    "\n",
    "import time\n",
    "\n",
    "vox_size = 32\n",
    "latent_dim = 256\n",
    "con_dim = 32\n",
    "\n",
    "batch_len = len(train_sdf_dataloader)\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, num_epoch):\n",
    "    \n",
    "    loss_list = []\n",
    "    loss_batch = []\n",
    "    \n",
    "    print(\"Epoch: \", epoch)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i, data in enumerate(train_sdf_dataloader):\n",
    "        \n",
    "        ####################\n",
    "        # Data preparation #\n",
    "        ####################\n",
    "        \n",
    "        # b x 1 x 256 x 256\n",
    "        depth_img = data['depth_img'].to(device)\n",
    "        # b x 128 x 1 x 1\n",
    "        z = discrete_encoder(depth_img)\n",
    "        # b x n x 3\n",
    "        # DO NOT scale by np.sqrt(3)\n",
    "        sample_pt = data['sample_pt']\n",
    "        # b x n x 1\n",
    "        sample_sdf = data['sample_sdf']\n",
    "        \n",
    "        # b x 16 x 64 x 64 x 64\n",
    "        target_vox = data['target_vox'].to(device)\n",
    "        vox_feature = unet(target_vox, z)\n",
    "        #vox_feature = unet(torch.sigmoid(discrete_decoder(mapping(z))))\n",
    "        \n",
    "        ####################\n",
    "        # indexing context #\n",
    "        ####################\n",
    "        \n",
    "        # stay with cpu for v-ram efficiency\n",
    "        sample_pt_normalized = sample_pt + torch.tensor([0.5, 0.5, 0.5])\n",
    "        # (0, vox_size-1)\n",
    "        sample_pt_scale = torch.clamp(sample_pt_normalized* (vox_size-1), 0, (vox_size-1)-1e-5)\n",
    "        # (0, vox_size-2)\n",
    "        sample_pt_query = torch.clamp((sample_pt_scale).int(), 0, (vox_size-2))\n",
    "        sample_pt_distance = sample_pt_scale - sample_pt_query\n",
    "        \n",
    "        context = getContext(sample_pt_query, vox_feature)\n",
    "        \n",
    "        dx = sample_pt_distance[:, :, 0].unsqueeze(1)\n",
    "        dy = sample_pt_distance[:, :, 1].unsqueeze(1)\n",
    "        dz = sample_pt_distance[:, :, 2].unsqueeze(1)\n",
    "        # local feature\n",
    "        con = trilinearInterpolation(context, dx, dy, dz)\n",
    "        \n",
    "        ################################\n",
    "        # Reshape input & forward pass #\n",
    "        ################################\n",
    "        \n",
    "        sample_pt = sample_pt.transpose(-1, -2).to(device)\n",
    "        con = con.to(device)\n",
    "        z = z.squeeze(-1).squeeze(-1).repeat(1, 1, sample_size)\n",
    "        sample_sdf = sample_sdf.transpose(-1, -2).to(device)\n",
    "        \n",
    "        \n",
    "        sample_pt = sample_pt.transpose(-1, -2).reshape(-1, 3)\n",
    "        con = con.transpose(-1, -2).reshape(-1, con_dim)\n",
    "        z = z.transpose(-1, -2).reshape(-1, latent_dim)\n",
    "        sample_sdf = sample_sdf.transpose(-1, -2).reshape(-1, 1)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_sdf = model(sample_pt, con, z)\n",
    "        \n",
    "        loss_l1 = l1loss(pred_sdf, sample_sdf)\n",
    "        \n",
    "        loss = loss_l1\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        loss_list.append(loss_l1.item())\n",
    "        loss_batch.append(loss_l1.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        if count != 0 and count % 10 == 0:\n",
    "            loss_batch_avg = np.average(loss_batch)\n",
    "            \n",
    "            print(\"Batch: \", count, \", l1 Loss: \", loss_batch_avg, \", Time: %s s\" % (time.time() - start_time))\n",
    "            \n",
    "            if count % 500 == 0:\n",
    "                torch.save(model.state_dict(), continuous_model_path)\n",
    "                \n",
    "            loss_batch.clear()\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "    print(\"Epoch: \", epoch, ', l1 loss: ', np.average(loss_list))\n",
    "    \n",
    "    loss_list.clear()\n",
    "    \n",
    "    torch.save(model.state_dict(), continuous_model_path)\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'continuous_state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    })\n",
    "    \n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
